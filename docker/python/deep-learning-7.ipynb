{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画像データを行列計算可能な形で取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h) // stride + 1\n",
    "    out_w = (H + 2*pad - filter_w) // stride + 1\n",
    "    \n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "    \n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "    \n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 入力データの形状（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h) // stride + 1\n",
    "    out_w = (H + 2*pad - filter_w) // stride + 1\n",
    "    col = col.shape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "    \n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    \n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "            \n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 畳み込みレイヤーの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int( (H + 2*self.pad - FH) // self.stride + 1)\n",
    "        out_w = int( (H + 2*self.pad - FW) // self.stride + 1)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).tranpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "        \n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.tranpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        \n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stribe, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int( 1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int( 1 + (H - self.pool_w) / self.stride)\n",
    "        \n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "        \n",
    "        out = np.max(col, axis=1)\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arrange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleConvNetの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                conv_param={'filter_num':30, 'filter_size':5,\n",
    "                           'pad':0, 'stribe':1},\n",
    "                hidden_size = 100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stribe = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2 * filter_pad) / filter_stribe + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        \n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        dout =1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.29993696589\n",
      "=== epoch:1, train acc:0.14, test acc:0.165 ===\n",
      "train loss:2.29717100526\n",
      "train loss:2.2944046537\n",
      "train loss:2.28767349866\n",
      "train loss:2.28033135279\n",
      "train loss:2.26704024417\n",
      "train loss:2.25749764204\n",
      "train loss:2.24049559927\n",
      "train loss:2.21821033277\n",
      "train loss:2.20644971278\n",
      "train loss:2.1745852206\n",
      "train loss:2.13560018154\n",
      "train loss:2.07765281076\n",
      "train loss:2.04075207727\n",
      "train loss:1.97449239429\n",
      "train loss:1.96120903056\n",
      "train loss:1.91139548463\n",
      "train loss:1.82502189589\n",
      "train loss:1.75965028629\n",
      "train loss:1.62945909698\n",
      "train loss:1.48345304368\n",
      "train loss:1.50533188447\n",
      "train loss:1.43209258693\n",
      "train loss:1.35092413638\n",
      "train loss:1.30974600368\n",
      "train loss:1.13756323075\n",
      "train loss:1.01462693371\n",
      "train loss:1.07021044927\n",
      "train loss:0.943368370901\n",
      "train loss:0.902110550837\n",
      "train loss:0.912288658573\n",
      "train loss:0.794591889888\n",
      "train loss:0.86711345809\n",
      "train loss:0.715325636843\n",
      "train loss:0.675565073459\n",
      "train loss:0.754076855681\n",
      "train loss:0.718836179944\n",
      "train loss:0.730842246379\n",
      "train loss:1.01073776755\n",
      "train loss:0.568314375204\n",
      "train loss:0.680462798075\n",
      "train loss:0.517596700405\n",
      "train loss:0.4809519976\n",
      "train loss:0.683361036058\n",
      "train loss:0.556573347369\n",
      "train loss:0.748596015596\n",
      "train loss:0.599457291215\n",
      "train loss:0.500410144321\n",
      "train loss:0.742125778161\n",
      "train loss:0.471821517476\n",
      "train loss:0.595968517544\n",
      "=== epoch:2, train acc:0.837, test acc:0.81 ===\n",
      "train loss:0.530831135151\n",
      "train loss:0.392243199325\n",
      "train loss:0.542045417006\n",
      "train loss:0.605798406606\n",
      "train loss:0.409309128757\n",
      "train loss:0.569077773838\n",
      "train loss:0.488939099584\n",
      "train loss:0.450785092351\n",
      "train loss:0.440830818596\n",
      "train loss:0.326282398902\n",
      "train loss:0.368799432479\n",
      "train loss:0.55733050857\n",
      "train loss:0.561828633312\n",
      "train loss:0.477050047151\n",
      "train loss:0.474800423072\n",
      "train loss:0.504049953432\n",
      "train loss:0.559054741438\n",
      "train loss:0.517303949253\n",
      "train loss:0.430370622777\n",
      "train loss:0.408892760529\n",
      "train loss:0.393101341137\n",
      "train loss:0.375071574022\n",
      "train loss:0.486358024257\n",
      "train loss:0.468421118108\n",
      "train loss:0.585504029421\n",
      "train loss:0.31127774787\n",
      "train loss:0.440510329801\n",
      "train loss:0.314667220498\n",
      "train loss:0.334183115658\n",
      "train loss:0.38511872163\n",
      "train loss:0.47798445581\n",
      "train loss:0.416079897559\n",
      "train loss:0.260423148866\n",
      "train loss:0.341079558821\n",
      "train loss:0.399439270229\n",
      "train loss:0.224743874674\n",
      "train loss:0.51927737647\n",
      "train loss:0.404255400358\n",
      "train loss:0.467720213798\n",
      "train loss:0.362856666289\n",
      "train loss:0.410300688272\n",
      "train loss:0.421010171526\n",
      "train loss:0.194724997774\n",
      "train loss:0.397204919575\n",
      "train loss:0.354745350841\n",
      "train loss:0.297173318647\n",
      "train loss:0.22576551701\n",
      "train loss:0.394096130539\n",
      "train loss:0.322584866689\n",
      "train loss:0.420658676117\n",
      "=== epoch:3, train acc:0.891, test acc:0.863 ===\n",
      "train loss:0.403479744734\n",
      "train loss:0.314084238169\n",
      "train loss:0.461337678756\n",
      "train loss:0.227951234532\n",
      "train loss:0.305160557684\n",
      "train loss:0.427102878213\n",
      "train loss:0.30192773148\n",
      "train loss:0.300850886645\n",
      "train loss:0.342305779736\n",
      "train loss:0.248171370354\n",
      "train loss:0.458278560353\n",
      "train loss:0.45876211803\n",
      "train loss:0.26217674179\n",
      "train loss:0.430415202897\n",
      "train loss:0.285428816762\n",
      "train loss:0.260971352604\n",
      "train loss:0.249759617715\n",
      "train loss:0.256362975899\n",
      "train loss:0.375497667492\n",
      "train loss:0.368308346955\n",
      "train loss:0.167159210507\n",
      "train loss:0.276070883807\n",
      "train loss:0.288383878654\n",
      "train loss:0.36874161443\n",
      "train loss:0.381510432236\n",
      "train loss:0.358984482329\n",
      "train loss:0.192169970703\n",
      "train loss:0.238575482718\n",
      "train loss:0.303483121763\n",
      "train loss:0.22539807724\n",
      "train loss:0.348466031179\n",
      "train loss:0.290576312548\n",
      "train loss:0.260863852214\n",
      "train loss:0.243907686597\n",
      "train loss:0.215772949033\n",
      "train loss:0.322994354645\n",
      "train loss:0.320354246531\n",
      "train loss:0.307310564558\n",
      "train loss:0.297779655852\n",
      "train loss:0.297632366671\n",
      "train loss:0.37666630745\n",
      "train loss:0.318908685215\n",
      "train loss:0.273340311079\n",
      "train loss:0.425215418308\n",
      "train loss:0.215599651765\n",
      "train loss:0.307940093608\n",
      "train loss:0.216546500381\n",
      "train loss:0.329087812453\n",
      "train loss:0.148208167415\n",
      "train loss:0.384742933352\n",
      "=== epoch:4, train acc:0.903, test acc:0.886 ===\n",
      "train loss:0.250911130748\n",
      "train loss:0.238015655401\n",
      "train loss:0.330228660771\n",
      "train loss:0.248197902221\n",
      "train loss:0.258920999824\n",
      "train loss:0.312412404671\n",
      "train loss:0.205786110545\n",
      "train loss:0.1927491596\n",
      "train loss:0.336983306808\n",
      "train loss:0.223348517902\n",
      "train loss:0.135138690497\n",
      "train loss:0.224743804838\n",
      "train loss:0.17509576746\n",
      "train loss:0.168458169973\n",
      "train loss:0.128838052047\n",
      "train loss:0.427964281287\n",
      "train loss:0.192860121108\n",
      "train loss:0.188870176572\n",
      "train loss:0.121886110109\n",
      "train loss:0.357928785852\n",
      "train loss:0.163405812123\n",
      "train loss:0.190234612487\n",
      "train loss:0.19722151449\n",
      "train loss:0.233634146601\n",
      "train loss:0.323170353043\n",
      "train loss:0.272299096475\n",
      "train loss:0.221972875979\n",
      "train loss:0.238664211745\n",
      "train loss:0.265375122405\n",
      "train loss:0.189114891379\n",
      "train loss:0.19720772257\n",
      "train loss:0.237372403058\n",
      "train loss:0.297051525739\n",
      "train loss:0.167168469137\n",
      "train loss:0.243319091928\n",
      "train loss:0.211268652062\n",
      "train loss:0.238213198688\n",
      "train loss:0.269186423906\n",
      "train loss:0.325171115191\n",
      "train loss:0.239819078533\n",
      "train loss:0.156253981633\n",
      "train loss:0.217018267376\n",
      "train loss:0.162721580967\n",
      "train loss:0.181905928594\n",
      "train loss:0.29145347803\n",
      "train loss:0.244098263016\n",
      "train loss:0.208714931438\n",
      "train loss:0.23244221078\n",
      "train loss:0.174126579416\n",
      "train loss:0.27076138316\n",
      "=== epoch:5, train acc:0.915, test acc:0.902 ===\n",
      "train loss:0.26797113445\n",
      "train loss:0.146096569253\n",
      "train loss:0.241858781198\n",
      "train loss:0.183946126324\n",
      "train loss:0.374960478741\n",
      "train loss:0.149324749207\n",
      "train loss:0.154998065642\n",
      "train loss:0.209234063725\n",
      "train loss:0.164667594456\n",
      "train loss:0.177594417003\n",
      "train loss:0.155535673721\n",
      "train loss:0.408046918149\n",
      "train loss:0.265845741878\n",
      "train loss:0.213996146175\n",
      "train loss:0.218188551524\n",
      "train loss:0.179210367839\n",
      "train loss:0.127517254952\n",
      "train loss:0.181231632015\n",
      "train loss:0.288528251364\n",
      "train loss:0.196587146372\n",
      "train loss:0.186598188705\n",
      "train loss:0.14026953242\n",
      "train loss:0.168511385345\n",
      "train loss:0.251414313155\n",
      "train loss:0.111923005536\n",
      "train loss:0.157675620934\n",
      "train loss:0.153116498606\n",
      "train loss:0.203132738259\n",
      "train loss:0.168085802106\n",
      "train loss:0.101036516102\n",
      "train loss:0.20430916334\n",
      "train loss:0.208301980169\n",
      "train loss:0.255457123128\n",
      "train loss:0.155905926714\n",
      "train loss:0.277254839664\n",
      "train loss:0.10652530442\n",
      "train loss:0.32511795349\n",
      "train loss:0.234883802464\n",
      "train loss:0.308007083136\n",
      "train loss:0.170668063915\n",
      "train loss:0.194981789511\n",
      "train loss:0.207555360737\n",
      "train loss:0.213673393157\n",
      "train loss:0.179892940667\n",
      "train loss:0.310501103982\n",
      "train loss:0.28553897545\n",
      "train loss:0.196581393001\n",
      "train loss:0.121179978562\n",
      "train loss:0.146303838313\n",
      "train loss:0.225730771849\n",
      "=== epoch:6, train acc:0.933, test acc:0.914 ===\n",
      "train loss:0.164599360723\n",
      "train loss:0.0987491738978\n",
      "train loss:0.117952655838\n",
      "train loss:0.161154595155\n",
      "train loss:0.27149567317\n",
      "train loss:0.148464395336\n",
      "train loss:0.139912235923\n",
      "train loss:0.154933551552\n",
      "train loss:0.150560501119\n",
      "train loss:0.127688971809\n",
      "train loss:0.166865801757\n",
      "train loss:0.207243762249\n",
      "train loss:0.267401706473\n",
      "train loss:0.164133669514\n",
      "train loss:0.217420992986\n",
      "train loss:0.118189620179\n",
      "train loss:0.0844775517718\n",
      "train loss:0.121386824288\n",
      "train loss:0.247833580139\n",
      "train loss:0.173687852896\n",
      "train loss:0.139090722602\n",
      "train loss:0.154475393714\n",
      "train loss:0.0947645661521\n",
      "train loss:0.16019114973\n",
      "train loss:0.200146915388\n",
      "train loss:0.145120683025\n",
      "train loss:0.197210741243\n",
      "train loss:0.11141975624\n",
      "train loss:0.116720604984\n",
      "train loss:0.115740479449\n",
      "train loss:0.146201155421\n",
      "train loss:0.281789296851\n",
      "train loss:0.222302843778\n",
      "train loss:0.169674691099\n",
      "train loss:0.218152539467\n",
      "train loss:0.207563043863\n",
      "train loss:0.174143896055\n",
      "train loss:0.238733561554\n",
      "train loss:0.179305978017\n",
      "train loss:0.145908917197\n",
      "train loss:0.294902116973\n",
      "train loss:0.214977968123\n",
      "train loss:0.124371858247\n",
      "train loss:0.162499968467\n",
      "train loss:0.179841970961\n",
      "train loss:0.152751562794\n",
      "train loss:0.112712439071\n",
      "train loss:0.117841697081\n",
      "train loss:0.223043797322\n",
      "train loss:0.0838653023955\n",
      "=== epoch:7, train acc:0.935, test acc:0.915 ===\n",
      "train loss:0.1532511926\n",
      "train loss:0.167542525939\n",
      "train loss:0.199308843534\n",
      "train loss:0.122379911874\n",
      "train loss:0.0752734156185\n",
      "train loss:0.208585291488\n",
      "train loss:0.0830928148247\n",
      "train loss:0.0931396779557\n",
      "train loss:0.0956984900395\n",
      "train loss:0.1425390296\n",
      "train loss:0.241290098264\n",
      "train loss:0.208650438443\n",
      "train loss:0.067271336408\n",
      "train loss:0.136067509405\n",
      "train loss:0.0810061871098\n",
      "train loss:0.154099339173\n",
      "train loss:0.0625911826677\n",
      "train loss:0.168182299394\n",
      "train loss:0.133900160299\n",
      "train loss:0.137407872623\n",
      "train loss:0.150736235193\n",
      "train loss:0.386446382143\n",
      "train loss:0.11339308554\n",
      "train loss:0.13279863934\n",
      "train loss:0.112289440119\n",
      "train loss:0.078568113081\n",
      "train loss:0.156629414457\n",
      "train loss:0.207996149973\n",
      "train loss:0.178887829405\n",
      "train loss:0.209761010341\n",
      "train loss:0.0840490873246\n",
      "train loss:0.239898758903\n",
      "train loss:0.0653394321918\n",
      "train loss:0.104281467344\n",
      "train loss:0.106600990843\n",
      "train loss:0.152095286934\n",
      "train loss:0.100082691515\n",
      "train loss:0.0593251897844\n",
      "train loss:0.144892545031\n",
      "train loss:0.128404245956\n",
      "train loss:0.321632365946\n",
      "train loss:0.0845996061656\n",
      "train loss:0.068632438411\n",
      "train loss:0.0992365324711\n",
      "train loss:0.230551022323\n",
      "train loss:0.0978106130937\n",
      "train loss:0.130863391211\n",
      "train loss:0.109189944973\n",
      "train loss:0.0883663711931\n",
      "train loss:0.103196545262\n",
      "=== epoch:8, train acc:0.952, test acc:0.931 ===\n",
      "train loss:0.126479534215\n",
      "train loss:0.0740682691542\n",
      "train loss:0.161423244537\n",
      "train loss:0.15823959031\n",
      "train loss:0.0855032876556\n",
      "train loss:0.133271249048\n",
      "train loss:0.184426684936\n",
      "train loss:0.180478406763\n",
      "train loss:0.192984890897\n",
      "train loss:0.131224671052\n",
      "train loss:0.168149439031\n",
      "train loss:0.112106643039\n",
      "train loss:0.181162804429\n",
      "train loss:0.101861033696\n",
      "train loss:0.075956714353\n",
      "train loss:0.106942409689\n",
      "train loss:0.163112040818\n",
      "train loss:0.138300675805\n",
      "train loss:0.0505833832825\n",
      "train loss:0.0858231047671\n",
      "train loss:0.23731469906\n",
      "train loss:0.109914132975\n",
      "train loss:0.0995501996994\n",
      "train loss:0.153289021723\n",
      "train loss:0.179733084595\n",
      "train loss:0.0921949126282\n",
      "train loss:0.183662443899\n",
      "train loss:0.0722054826681\n",
      "train loss:0.150752151814\n",
      "train loss:0.146802235485\n",
      "train loss:0.130955761214\n",
      "train loss:0.0760147366641\n",
      "train loss:0.13129355522\n",
      "train loss:0.112663123672\n",
      "train loss:0.207559560902\n",
      "train loss:0.116306419357\n",
      "train loss:0.0860693623657\n",
      "train loss:0.0507975155844\n",
      "train loss:0.102209679953\n",
      "train loss:0.177266397019\n",
      "train loss:0.0960149048076\n",
      "train loss:0.144510799087\n",
      "train loss:0.156189366439\n",
      "train loss:0.155151138125\n",
      "train loss:0.147091080657\n",
      "train loss:0.0542668766294\n",
      "train loss:0.102007775403\n",
      "train loss:0.0667344682644\n",
      "train loss:0.137834226277\n",
      "train loss:0.177990850838\n",
      "=== epoch:9, train acc:0.956, test acc:0.942 ===\n",
      "train loss:0.144069534167\n",
      "train loss:0.176825397618\n",
      "train loss:0.0439117783436\n",
      "train loss:0.0848544365068\n",
      "train loss:0.0466928467291\n",
      "train loss:0.0881053913144\n",
      "train loss:0.108108734599\n",
      "train loss:0.117816930082\n",
      "train loss:0.164798826205\n",
      "train loss:0.110284762006\n",
      "train loss:0.0421103200389\n",
      "train loss:0.130531028885\n",
      "train loss:0.0387791817388\n",
      "train loss:0.118509703013\n",
      "train loss:0.0537610475834\n",
      "train loss:0.0575270219655\n",
      "train loss:0.0490251410597\n",
      "train loss:0.0726437065506\n",
      "train loss:0.0814818817436\n",
      "train loss:0.0525738643125\n",
      "train loss:0.0912595974732\n",
      "train loss:0.11043446214\n",
      "train loss:0.0639951425856\n",
      "train loss:0.069430953421\n",
      "train loss:0.0921645277782\n",
      "train loss:0.0978468273399\n",
      "train loss:0.0618828726347\n",
      "train loss:0.0393184983406\n",
      "train loss:0.0920329985352\n",
      "train loss:0.103594116622\n",
      "train loss:0.0925899872682\n",
      "train loss:0.0572932617462\n",
      "train loss:0.0887159429797\n",
      "train loss:0.123206884072\n",
      "train loss:0.0608059294982\n",
      "train loss:0.127004244383\n",
      "train loss:0.147635767816\n",
      "train loss:0.111823690344\n",
      "train loss:0.0481846949006\n",
      "train loss:0.139261309275\n",
      "train loss:0.0821713741299\n",
      "train loss:0.139338882422\n",
      "train loss:0.0546971754798\n",
      "train loss:0.0699165753772\n",
      "train loss:0.046178975851\n",
      "train loss:0.0816689297532\n",
      "train loss:0.100719129519\n",
      "train loss:0.0663527615971\n",
      "train loss:0.103335970648\n",
      "train loss:0.122114871334\n",
      "=== epoch:10, train acc:0.966, test acc:0.95 ===\n",
      "train loss:0.108207040196\n",
      "train loss:0.0564895482077\n",
      "train loss:0.0578730557011\n",
      "train loss:0.129138134379\n",
      "train loss:0.0728518247292\n",
      "train loss:0.1036606117\n",
      "train loss:0.0744345289386\n",
      "train loss:0.0788115969076\n",
      "train loss:0.0631998876873\n",
      "train loss:0.0643170909019\n",
      "train loss:0.0819089923255\n",
      "train loss:0.107694557115\n",
      "train loss:0.0446223456959\n",
      "train loss:0.087174509119\n",
      "train loss:0.17981993627\n",
      "train loss:0.0526770616502\n",
      "train loss:0.0478431297387\n",
      "train loss:0.0867740351984\n",
      "train loss:0.106658345056\n",
      "train loss:0.105972052651\n",
      "train loss:0.0368269449888\n",
      "train loss:0.0753602692401\n",
      "train loss:0.0635009417878\n",
      "train loss:0.105957859975\n",
      "train loss:0.0423243635276\n",
      "train loss:0.0693675607336\n",
      "train loss:0.0482355672485\n",
      "train loss:0.064459006789\n",
      "train loss:0.176002815063\n",
      "train loss:0.105084988789\n",
      "train loss:0.112385316723\n",
      "train loss:0.160426883636\n",
      "train loss:0.0636786701616\n",
      "train loss:0.0637833650791\n",
      "train loss:0.0751340660608\n",
      "train loss:0.107459126935\n",
      "train loss:0.0910822669601\n",
      "train loss:0.0514497785629\n",
      "train loss:0.163623719255\n",
      "train loss:0.169581565947\n",
      "train loss:0.0761089886562\n",
      "train loss:0.0741522715363\n",
      "train loss:0.0971106367515\n",
      "train loss:0.0347278948606\n",
      "train loss:0.0735335575794\n",
      "train loss:0.131414854044\n",
      "train loss:0.102889973396\n",
      "train loss:0.0640726218034\n",
      "train loss:0.0609473703343\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/matplotlib/__init__.py:1357: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEPCAYAAACgFqixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ddf2b72e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "%matplotlib inline\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 10\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "#network.save_params(\"params.pkl\")\n",
    "#print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
